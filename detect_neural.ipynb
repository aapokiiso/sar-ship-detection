{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkFb65CGqG4f"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision pycocotools\n",
        "!pip install torch_xla"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5j6ySGgqPkQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.io import read_image\n",
        "from pycocotools.coco import COCO\n",
        "\n",
        "class COCODataset(Dataset):\n",
        "    def __init__(self, root, annotation_file, transforms=None):\n",
        "        self.root = root\n",
        "        self.coco = COCO(annotation_file)\n",
        "        self.ids = list(self.coco.imgs.keys())\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_id = self.ids[index]\n",
        "        img_info = self.coco.loadImgs(img_id)[0]\n",
        "        img_path = os.path.join(self.root, img_info['file_name'])\n",
        "        image = read_image(img_path).float() / 255.0  # Normalize to [0,1]\n",
        "\n",
        "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
        "        anns = self.coco.loadAnns(ann_ids)\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        for ann in anns:\n",
        "            bbox = ann['bbox']  # [x_min, y_min, width, height]\n",
        "            boxes.append([bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]])\n",
        "            labels.append(ann['category_id'])\n",
        "\n",
        "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.tensor(labels, dtype=torch.int64)\n",
        "        target = {\"boxes\": boxes, \"labels\": labels}\n",
        "\n",
        "        if self.transforms:\n",
        "            image, target = self.transforms(image, target)\n",
        "\n",
        "        return image, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o451dN4dug_v"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from pycocotools.coco import COCO\n",
        "import os\n",
        "\n",
        "# Paths to images and annotation files\n",
        "root_dir = \"/content/drive/MyDrive/Colab Notebooks/HRSID_JPG/JPEGImages/\"\n",
        "annotation_file = \"/content/drive/MyDrive/Colab Notebooks/HRSID_JPG/annotations/train2017.json\"\n",
        "\n",
        "# Load the COCO annotations\n",
        "coco = COCO(annotation_file)\n",
        "\n",
        "# Filter valid images\n",
        "valid_images = []\n",
        "valid_annotations = []\n",
        "\n",
        "for img_id in coco.imgs:\n",
        "    img_info = coco.loadImgs(img_id)[0]\n",
        "    file_path = os.path.join(root_dir, img_info['file_name'])\n",
        "    if os.path.exists(file_path):\n",
        "        valid_images.append(img_info)\n",
        "        valid_annotations.extend([ann for ann in coco.anns.values() if ann['image_id'] == img_info['id']])\n",
        "    else:\n",
        "        print(f\"Skipping missing file: {img_info['file_name']}\")\n",
        "\n",
        "# Create a new annotation dictionary\n",
        "coco_data_filtered = {\n",
        "    \"images\": valid_images,\n",
        "    \"annotations\": valid_annotations,\n",
        "    \"categories\": coco.dataset['categories']\n",
        "}\n",
        "\n",
        "# Save the filtered dataset\n",
        "filtered_annotation_file = \"/content/drive/MyDrive/Colab Notebooks/HRSID_JPG/annotations_filtered.json\"\n",
        "with open(filtered_annotation_file, 'w') as f:\n",
        "    import json\n",
        "    json.dump(coco_data_filtered, f)\n",
        "\n",
        "print(\"Filtered annotations saved.\")\n",
        "\n",
        "filtered_annotation_file = \"/content/drive/MyDrive/Colab Notebooks/HRSID_JPG/annotations_filtered.json\"\n",
        "dataset = COCODataset(root=root_dir, annotation_file=filtered_annotation_file)\n",
        "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5y70Qdd9NBQ"
      },
      "outputs": [],
      "source": [
        "## Uncomment this section for training (the model is saved after one training)\n",
        "\n",
        "#import torch\n",
        "#from torch.optim import Adam\n",
        "#\n",
        "#device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "#\n",
        "#print(f\"Using device: {device}\")\n",
        "#model.to(device)\n",
        "#\n",
        "## Optimizer\n",
        "#optimizer = Adam(model.parameters(), lr=0.001)\n",
        "#\n",
        "## Training loop\n",
        "#num_epochs = 10\n",
        "#for epoch in range(num_epochs):\n",
        "#    model.train()\n",
        "#    epoch_loss = 0.0  # Track total loss for the epoch\n",
        "#\n",
        "#    for batch_idx, (images, targets) in enumerate(dataloader):\n",
        "#        images = [image.to(device) for image in images]\n",
        "#        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "#\n",
        "#        loss_dict = model(images, targets)\n",
        "#        losses = sum(loss for loss in loss_dict.values())\n",
        "#\n",
        "#        optimizer.zero_grad()\n",
        "#        losses.backward()\n",
        "#        optimizer.step()\n",
        "#\n",
        "#        epoch_loss += losses.item()\n",
        "#\n",
        "#    print(f\"Epoch {epoch + 1}, Loss: {epoch_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAmgtSQgsVa8"
      },
      "outputs": [],
      "source": [
        "# Load the model after it's saved\n",
        "!ls /content\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "\n",
        "# Define the device\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# Recreate the model\n",
        "# Load a pretrained Faster R-CNN model\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "# Replace the classifier head with the number of categories\n",
        "num_classes = len(dataset.coco.getCatIds()) + 1  # Add 1 for the background class\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "# Load the saved weights\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/faster_rcnn_coco.pth\", map_location=device))\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.to(device)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZuQxIafXBtYi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Load a batch of images from the dataloader\n",
        "images, targets = next(iter(dataloader))\n",
        "images = [img.to(device) for img in images]\n",
        "\n",
        "with torch.no_grad():\n",
        "    predictions = model(images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yw-nYawxBzZl"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "# Define a function to visualize the predictions\n",
        "def visualize_predictions(image, prediction, threshold=0.5):\n",
        "    # Convert image tensor to numpy\n",
        "    image = image.permute(1, 2, 0).cpu().numpy()\n",
        "\n",
        "    # Create the plot\n",
        "    fig, ax = plt.subplots(1, figsize=(12, 8))\n",
        "    ax.imshow(image)\n",
        "\n",
        "    # Draw bounding boxes\n",
        "    for box, label, score in zip(prediction['boxes'], prediction['labels'], prediction['scores']):\n",
        "        if score >= threshold:\n",
        "            x_min, y_min, x_max, y_max = box.cpu().numpy()\n",
        "            width, height = x_max - x_min, y_max - y_min\n",
        "\n",
        "            # Create a rectangle\n",
        "            rect = patches.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor='r', facecolor='none')\n",
        "            ax.add_patch(rect)\n",
        "\n",
        "            # Add a confidence score\n",
        "            ax.text(x_min, y_min - 5, f'{score:.2f}', color='red', fontsize=12, weight='bold')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Visualize the first image and its prediction\n",
        "visualize_predictions(images[3], predictions[3])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syToatBA5SnJ"
      },
      "outputs": [],
      "source": [
        "!pip install rasterio\n",
        "import torch\n",
        "import rasterio\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "image_path = \"/content/drive/MyDrive/pp_20241030T091501_ICEYE_GRD_SC_244270_20220207T192226.tif\"\n",
        "\n",
        "# Load the image\n",
        "with rasterio.open(image_path) as src:\n",
        "    img = src.read(1)  # Read the first band (grayscale)\n",
        "    print(f\"Image shape: {img.shape}, dtype: {img.dtype}\")\n",
        "\n",
        "# Apply gamma correction\n",
        "gamma = 1.5\n",
        "img = np.clip(img / 255.0, 0, 1)  # Normalize\n",
        "img = np.power(img, gamma) * 255  # Gamma correction\n",
        "\n",
        "# Normalize image for model input\n",
        "min_val, max_val = np.min(img), np.max(img)\n",
        "normalized_img = (img - min_val) / (max_val - min_val)\n",
        "\n",
        "# Convert to tensor\n",
        "image_tensor = torch.tensor(normalized_img, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "# Move to device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "image_tensor = image_tensor.to(device)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    prediction = model(image_tensor)[0]\n",
        "\n",
        "# Convert grayscale image to 3-channel format for visualization\n",
        "image_tensor_vis = image_tensor.repeat(1, 3, 1, 1).squeeze(0)  # (1, 1, H, W) → (3, H, W)\n",
        "visualize_predictions(image_tensor_vis, prediction, threshold=0.5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZZifmVR85Js"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
